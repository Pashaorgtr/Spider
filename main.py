#!/usr/bin/env python3
"""
Spider Domain Crawler - Ana Ã§alÄ±ÅŸtÄ±rma dosyasÄ±

Copyright (c) 2025 Hasan Yasin YaÅŸar
Licensed under PSH 1.1 (Pasha Software License)

Bu yazÄ±lÄ±m ticari kullanÄ±m iÃ§in yasaktÄ±r.
Ticari lisans iÃ§in: yasin@pasha.org.tr
Detaylar: LICENSE.md
"""

import argparse
import json
import time
import os
from datetime import datetime
from modules.config import DEFAULT_EXCLUDED_EXTENSIONS
from domain_crawler import crawl_and_detect_domains, spider_crawl_domains, enhanced_spider_crawl_domains

def save_results(results, filename=None):
    """SonuÃ§larÄ± JSON dosyasÄ±na kaydet"""
    # Data klasÃ¶rÃ¼nÃ¼ oluÅŸtur (yoksa)
    data_dir = "data"
    os.makedirs(data_dir, exist_ok=True)
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"domain_results_{timestamp}.json"
    
    # Dosya yolunu data klasÃ¶rÃ¼ ile birleÅŸtir
    if not filename.startswith(data_dir):
        filepath = os.path.join(data_dir, filename)
    else:
        filepath = filename
    
    # SonuÃ§larÄ± gÃ¼zelleÅŸtir
    results['timestamp'] = datetime.now().isoformat()
    results['excluded_extensions'] = results.get('excluded_extensions', DEFAULT_EXCLUDED_EXTENSIONS)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ SonuÃ§lar kaydedildi: {filepath}")
    return filepath

def main():
    parser = argparse.ArgumentParser(description='Domain Crawler - URL\'lerden domain tespit eder')
    parser.add_argument('urls', nargs='+', help='Crawl edilecek URL\'ler')
    parser.add_argument('--spider', action='store_true', help='Spider crawl modu (zincirleme domain keÅŸfi)')
    parser.add_argument('--enhanced-spider', action='store_true', help='GeliÅŸmiÅŸ spider crawl (hem sayfa hem domain zincirleme)')
    parser.add_argument('--spider-depth', type=int, default=3, help='Spider crawl derinliÄŸi (varsayÄ±lan: 3)')
    parser.add_argument('--spider-domains-per-level', type=int, default=20, help='Seviye baÅŸÄ±na max domain (varsayÄ±lan: 20)')
    parser.add_argument('--spider-max-domains', type=int, default=100, help='Toplam max domain (varsayÄ±lan: 100)')
    parser.add_argument('--spider-max-pages-per-domain', type=int, default=50, help='Domain baÅŸÄ±na max sayfa (geliÅŸmiÅŸ spider iÃ§in, varsayÄ±lan: 50)')
    parser.add_argument('--crawler-delay', type=float, default=1.0, help='Crawler gecikme sÃ¼resi (varsayÄ±lan: 1.0)')
    parser.add_argument('--crawler-depth', type=int, default=2, help='Crawler derinliÄŸi (varsayÄ±lan: 2)')
    parser.add_argument('--crawler-max-urls', type=int, default=50, help='Crawler max URL (varsayÄ±lan: 50)')
    parser.add_argument('--detector-delay', type=float, default=0.5, help='Detector gecikme sÃ¼resi (varsayÄ±lan: 0.5)')
    parser.add_argument('--detector-timeout', type=int, default=10, help='Detector timeout (varsayÄ±lan: 10)')
    parser.add_argument('--no-validation', action='store_true', help='Domain doÄŸrulamasÄ±nÄ± devre dÄ±ÅŸÄ± bÄ±rak')
    parser.add_argument('--output', '-o', help='Ã‡Ä±ktÄ± dosyasÄ± adÄ± (data/ klasÃ¶rÃ¼ne kaydedilir)')
    parser.add_argument('--exclude-extensions', nargs='*', help='HariÃ§ tutulacak dosya uzantÄ±larÄ± (Ã¶rn: --exclude-extensions .pdf .jpg)')
    parser.add_argument('--include-all-extensions', action='store_true', help='TÃ¼m dosya uzantÄ±larÄ±nÄ± dahil et (filtreleme yapma)')
    parser.add_argument('--random-user-agent', action='store_true', help='Random user agent kullan (her istekte farklÄ±)')
    parser.add_argument('--random-user-agent-per-session', action='store_true', help='Oturum baÅŸÄ±na random user agent kullan')
    parser.add_argument('--proxy', nargs='*', help='KullanÄ±lacak proxy listesi (Ã¶rn: --proxy http://proxy1:8080 socks5://proxy2:1080)')
    parser.add_argument('--proxy-file', help='Proxy listesini iÃ§eren dosya yolu')
    parser.add_argument('--no-proxy-rotation', action='store_true', help='Proxy rotasyonunu devre dÄ±ÅŸÄ± bÄ±rak')
    parser.add_argument('--block-domains', nargs='*', help='Engellenecek domain listesi (Ã¶rn: --block-domains google.com facebook.com)')
    parser.add_argument('--block-domains-file', help='Engellenecek domain listesini iÃ§eren dosya yolu')
    parser.add_argument('--use-default-blocked-domains', action='store_true', help='VarsayÄ±lan engellenen domain listesini kullan')
    
    args = parser.parse_args()
    
    # Random user agent ayarlarÄ±
    use_random_user_agent = args.random_user_agent or args.random_user_agent_per_session
    if use_random_user_agent:
        # Config dosyasÄ±ndaki ayarlarÄ± gÃ¼ncelle
        from modules.config import RANDOM_USER_AGENT_PER_REQUEST, RANDOM_USER_AGENT_PER_SESSION
        import modules.config as config
        
        if args.random_user_agent_per_session:
            config.RANDOM_USER_AGENT_PER_SESSION = True
            config.RANDOM_USER_AGENT_PER_REQUEST = False
            print("ğŸ­ Random user agent modu: Oturum baÅŸÄ±na farklÄ± user agent")
        else:
            config.RANDOM_USER_AGENT_PER_SESSION = False
            config.RANDOM_USER_AGENT_PER_REQUEST = True
            print("ğŸ­ Random user agent modu: Her istekte farklÄ± user agent")
    else:
        print("ğŸ­ Sabit user agent: Tarassut 1.0")
    
    # Proxy ayarlarÄ±
    proxy_list = []
    use_proxy = False
    
    if args.proxy:
        proxy_list.extend(args.proxy)
        use_proxy = True
        print(f"ğŸŒ Proxy modu: Komut satÄ±rÄ±ndan {len(args.proxy)} proxy")
    
    if args.proxy_file:
        try:
            with open(args.proxy_file, 'r', encoding='utf-8') as f:
                file_proxies = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                proxy_list.extend(file_proxies)
                use_proxy = True
                print(f"ğŸŒ Proxy modu: Dosyadan {len(file_proxies)} proxy yÃ¼klendi")
        except FileNotFoundError:
            print(f"âŒ Proxy dosyasÄ± bulunamadÄ±: {args.proxy_file}")
            return 1
        except Exception as e:
            print(f"âŒ Proxy dosyasÄ± okuma hatasÄ±: {e}")
            return 1
    
    if use_proxy:
        # Config dosyasÄ±ndaki proxy ayarlarÄ±nÄ± gÃ¼ncelle
        import modules.config as config
        if args.no_proxy_rotation:
            config.PROXY_ROTATION = False
            print("ğŸŒ Proxy rotasyonu: KapalÄ±")
        else:
            config.PROXY_ROTATION = True
            print("ğŸŒ Proxy rotasyonu: AÃ§Ä±k")
        
        print(f"ğŸŒ Toplam proxy sayÄ±sÄ±: {len(proxy_list)}")
        for i, proxy in enumerate(proxy_list[:3], 1):  # Ä°lk 3 proxy'yi gÃ¶ster
            print(f"   {i}. {proxy}")
        if len(proxy_list) > 3:
            print(f"   ... ve {len(proxy_list) - 3} proxy daha")
    else:
        print("ğŸŒ Proxy kullanÄ±lmÄ±yor")
    
    # Domain engelleme ayarlarÄ±
    from modules.config import USE_DOMAIN_BLOCKING, DEFAULT_BLOCKED_DOMAINS
    blocked_domains = []
    use_domain_blocking = USE_DOMAIN_BLOCKING  # Config'den varsayÄ±lan deÄŸeri al
    
    # EÄŸer varsayÄ±lan domain engelleme aÃ§Ä±ksa, varsayÄ±lan domain'leri ekle
    if USE_DOMAIN_BLOCKING:
        blocked_domains.extend(DEFAULT_BLOCKED_DOMAINS)
        print(f"ğŸš« Domain engelleme: VarsayÄ±lan olarak aktif ({len(DEFAULT_BLOCKED_DOMAINS)} domain)")
    
    if args.block_domains:
        blocked_domains.extend(args.block_domains)
        use_domain_blocking = True
        print(f"ğŸš« Domain engelleme: Komut satÄ±rÄ±ndan {len(args.block_domains)} domain eklendi")
    
    if args.block_domains_file:
        try:
            with open(args.block_domains_file, 'r', encoding='utf-8') as f:
                file_domains = [line.strip().lower() for line in f if line.strip() and not line.startswith('#')]
                blocked_domains.extend(file_domains)
                use_domain_blocking = True
                print(f"ğŸš« Domain engelleme: Dosyadan {len(file_domains)} domain yÃ¼klendi")
        except FileNotFoundError:
            print(f"âŒ Domain engelleme dosyasÄ± bulunamadÄ±: {args.block_domains_file}")
            return 1
        except Exception as e:
            print(f"âŒ Domain engelleme dosyasÄ± okuma hatasÄ±: {e}")
            return 1
    
    if args.use_default_blocked_domains:
        # Zaten varsayÄ±lan olarak eklendiyse, sadece bilgi ver
        if not USE_DOMAIN_BLOCKING:
            blocked_domains.extend(DEFAULT_BLOCKED_DOMAINS)
            use_domain_blocking = True
        print(f"ğŸš« Domain engelleme: VarsayÄ±lan domain'ler manuel olarak eklendi")
    
    if use_domain_blocking:
        # DuplikatlarÄ± kaldÄ±r
        blocked_domains = list(set(blocked_domains))
        print(f"ğŸš« Toplam engellenen domain sayÄ±sÄ±: {len(blocked_domains)}")
        
        # Ä°lk birkaÃ§ domain'i gÃ¶ster
        for i, domain in enumerate(blocked_domains[:5], 1):
            print(f"   {i}. {domain}")
        if len(blocked_domains) > 5:
            print(f"   ... ve {len(blocked_domains) - 5} domain daha")
    else:
        print("ğŸš« Domain engelleme kullanÄ±lmÄ±yor")
    
    # Excluded extensions ayarlarÄ±
    if args.include_all_extensions:
        excluded_extensions = []
        print("ğŸ“„ TÃ¼m dosya uzantÄ±larÄ± dahil edilecek (filtreleme yok)")
    elif args.exclude_extensions is not None:
        excluded_extensions = args.exclude_extensions
        print(f"ğŸ“„ Ã–zel hariÃ§ tutulan uzantÄ±lar: {excluded_extensions}")
    else:
        excluded_extensions = DEFAULT_EXCLUDED_EXTENSIONS
        print(f"ğŸ“„ VarsayÄ±lan hariÃ§ tutulan uzantÄ±lar kullanÄ±lÄ±yor: {len(excluded_extensions)} adet")
    
    print(f"ğŸš€ Domain Crawler baÅŸlatÄ±lÄ±yor...")
    print(f"ğŸ“‹ Ä°ÅŸlenecek URL'ler: {len(args.urls)}")
    for i, url in enumerate(args.urls, 1):
        print(f"   {i}. {url}")
    
    start_time = time.time()
    
    try:
        if args.enhanced_spider:
            print(f"\nğŸ•¸ï¸  GELÄ°ÅMÄ°Å SPIDER CRAWL MODU (Sayfa + Domain Zincirleme)")
            results = enhanced_spider_crawl_domains(
                args.urls,
                max_depth=args.spider_depth,
                max_domains_per_level=args.spider_domains_per_level,
                max_total_domains=args.spider_max_domains,
                max_pages_per_domain=args.spider_max_pages_per_domain,
                excluded_extensions=excluded_extensions,
                use_random_user_agent=use_random_user_agent,
                use_proxy=use_proxy,
                proxy_list=proxy_list,
                blocked_domains=blocked_domains,
                use_domain_blocking=use_domain_blocking
            )
        elif args.spider:
            print(f"\nğŸ•¸ï¸  SPIDER CRAWL MODU (Sadece Domain Zincirleme)")
            results = spider_crawl_domains(
                args.urls,
                max_depth=args.spider_depth,
                max_domains_per_level=args.spider_domains_per_level,
                max_total_domains=args.spider_max_domains,
                excluded_extensions=excluded_extensions,
                use_random_user_agent=use_random_user_agent,
                use_proxy=use_proxy,
                proxy_list=proxy_list,
                blocked_domains=blocked_domains,
                use_domain_blocking=use_domain_blocking
            )
        else:
            print(f"\nğŸ” NORMAL CRAWL MODU")
            
            # Crawler ayarlarÄ±
            crawler_settings = {
                'delay': args.crawler_delay,
                'max_depth': args.crawler_depth,
                'max_urls': args.crawler_max_urls,
                'excluded_extensions': excluded_extensions,
                'use_random_user_agent': use_random_user_agent,
                'use_proxy': use_proxy,
                'proxy_list': proxy_list,
                'blocked_domains': blocked_domains,
                'use_domain_blocking': use_domain_blocking
            }
            
            # Detector ayarlarÄ±
            detector_settings = {
                'delay': args.detector_delay,
                'timeout': args.detector_timeout,
                'validate_domains': not args.no_validation,
                'use_random_user_agent': use_random_user_agent,
                'use_proxy': use_proxy,
                'proxy_list': proxy_list,
                'blocked_domains': blocked_domains,
                'use_domain_blocking': use_domain_blocking
            }
            
            crawl_results, domain_results, detector = crawl_and_detect_domains(
                args.urls, crawler_settings, detector_settings
            )
            
            # SonuÃ§larÄ± birleÅŸtir
            results = {
                **domain_results,
                'crawl_stats': crawl_results,
                'excluded_extensions': excluded_extensions
            }
        
        # Ã‡alÄ±ÅŸma sÃ¼resini hesapla
        execution_time = time.time() - start_time
        results['execution_time_seconds'] = round(execution_time, 2)
        
        print(f"\nâ±ï¸  Toplam Ã§alÄ±ÅŸma sÃ¼resi: {execution_time:.2f} saniye")
        
        # SonuÃ§larÄ± kaydet
        output_file = save_results(results, args.output)
        
        # Ã–zet bilgileri gÃ¶ster
        print(f"\nğŸ“Š Ã–ZET BÄ°LGÄ°LER:")
        print(f"   Toplam bulunan domain: {results['total_domains_found']}")
        print(f"   GeÃ§erli domain'ler: {results['total_valid_domains']}")
        print(f"   GeÃ§ersiz domain'ler: {results['total_invalid_domains']}")
        print(f"   HariÃ§ tutulan uzantÄ± sayÄ±sÄ±: {len(excluded_extensions)}")
        
        if args.enhanced_spider:
            print(f"   Toplam bulunan sayfa: {results.get('total_pages_found', 0)}")
            print(f"   Spider derinliÄŸi: {results['spider_depth']}")
            print(f"   Ä°ÅŸlenen domain sayÄ±sÄ±: {results['processed_domains']}")
            print(f"   Domain baÅŸÄ±na max sayfa: {results.get('max_pages_per_domain', 0)}")
            print(f"   Crawl stratejisi: {results.get('crawl_strategy', 'N/A')}")
        elif args.spider:
            print(f"   Spider derinliÄŸi: {results['spider_depth']}")
            print(f"   Ä°ÅŸlenen domain sayÄ±sÄ±: {results['processed_domains']}")
        
        # Ä°lk 10 domain'i gÃ¶ster
        if results['valid_domains']:
            print(f"\nğŸ¯ Ä°lk 10 geÃ§erli domain:")
            for i, domain in enumerate(results['valid_domains'][:10], 1):
                print(f"   {i:2d}. {domain}")
            
            if len(results['valid_domains']) > 10:
                print(f"   ... ve {len(results['valid_domains']) - 10} domain daha")
        
        # Enhanced spider iÃ§in sayfa Ã¶rnekleri gÃ¶ster
        if args.enhanced_spider and results.get('found_pages'):
            print(f"\nğŸ“„ Ä°lk 5 bulunan sayfa:")
            for i, page in enumerate(results['found_pages'][:5], 1):
                print(f"   {i:2d}. {page}")
            
            if len(results['found_pages']) > 5:
                print(f"   ... ve {len(results['found_pages']) - 5} sayfa daha")
        
        print(f"\nâœ… Ä°ÅŸlem baÅŸarÄ±yla tamamlandÄ±!")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu")
        return 1
    except Exception as e:
        print(f"\nâŒ Hata oluÅŸtu: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
