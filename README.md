# Spider Domain Crawler

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-PSH%201.1-red.svg)](LICENSE.md)
[![GitHub Issues](https://img.shields.io/github/issues/yourusername/spider-domain-crawler.svg)](https://github.com/Pashaorgtr/Spider/issues)
[![GitHub Stars](https://img.shields.io/github/stars/yourusername/spider-domain-crawler.svg)](https://github.com/Pashaorgtr/Spider/stargazers)

Web sayfalarÄ±ndan **zincirleme domain tespiti** yapan Python crawler yazÄ±lÄ±mÄ±.

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

```bash
# Projeyi klonla
git clone https://github.com/Pashaorgtr/Spider.git
cd Spider

# Virtual environment oluÅŸtur
python3 -m venv spider-venv
source spider-venv/bin/activate  # Linux/Mac
# spider-venv\Scripts\activate   # Windows

# KÃ¼tÃ¼phaneleri yÃ¼kle
pip install -r requirements.txt

# HÄ±zlÄ± test
python main.py https://pasha.org.tr
```

## ğŸ“‹ Ä°Ã§indekiler

- [Ã–zellikler](#Ã¶zellikler)
- [Kurulum](#kurulum)
- [KullanÄ±m](#kullanÄ±m)
- [Parametreler](#parametreler)
- [Ã–rnekler](#Ã¶rnekler)
- [KatkÄ±da Bulunma](#katkÄ±da-bulunma)
- [Lisans](#lisans)

## Proje YapÄ±sÄ±

```
spider/
â”œâ”€â”€ modules/                 # Ana modÃ¼ller
â”‚   â”œâ”€â”€ __init__.py         
â”‚   â”œâ”€â”€ url_crawler.py      # URL crawler
â”‚   â”œâ”€â”€ url_utils.py        # URL analiz araÃ§larÄ±
â”‚   â”œâ”€â”€ domain_detector.py  # Domain tespit modÃ¼lÃ¼
â”‚   â””â”€â”€ config.py           # KonfigÃ¼rasyon ve dosya uzantÄ±larÄ±
â”œâ”€â”€ data/                   # Ã‡Ä±ktÄ± dosyalarÄ± (JSON sonuÃ§larÄ±)
â”œâ”€â”€ main.py                 # Ana Ã§alÄ±ÅŸtÄ±rma dosyasÄ±
â”œâ”€â”€ domain_crawler.py       # Spider Domain Crawler fonksiyonlarÄ±
â”œâ”€â”€ requirements.txt        
â”œâ”€â”€ README.md              
â””â”€â”€ KULLANIM_KILAVUZU.md   
```

## Ã–zellikler

### ğŸ•¸ï¸ Spider Crawl (Ana Ã–zellik)
- **Zincirleme Crawling**: Bulunan domain'leri tekrar crawl eder
- **Ã‡ok Seviyeli**: Belirlenen derinlikte spider crawl yapar
- **Otomatik GeniÅŸleme**: Her seviyede yeni domain'ler bulur
- **AkÄ±llÄ± Filtreleme**: Daha Ã¶nce iÅŸlenmiÅŸ domain'leri atlar

### ğŸ“„ Dosya UzantÄ±sÄ± Filtreleme
- **VarsayÄ±lan Filtreleme**: 74 farklÄ± dosya uzantÄ±sÄ± otomatik hariÃ§ tutuluyor
- **Ã–zel Filtreleme**: Ä°stediÄŸiniz uzantÄ±larÄ± belirtebilirsiniz
- **Filtresiz Mod**: TÃ¼m dosya tÃ¼rlerini dahil edebilirsiniz
- **Performans**: Gereksiz dosyalar crawl edilmez

### ğŸ” Domain Tespiti
- **DNS DoÄŸrulama**: Sadece gerÃ§ek domain'ler kaydedilir
- **Otomatik Tespit**: HTML iÃ§eriÄŸinden domain Ã§Ä±karÄ±r
- **Temiz Ã‡Ä±ktÄ±**: Sadece domain listesi
- **Ã–zel User-Agent**: "Tarassut 1.0" kimliÄŸi ile istekler
- **Random User-Agent**: Ä°steÄŸe baÄŸlÄ± random user agent desteÄŸi
- **Proxy DesteÄŸi**: HTTP/HTTPS/SOCKS proxy desteÄŸi ve rotasyon
- **Domain Engelleme**: Ä°stenmeyen domain'lerin otomatik atlanmasÄ±

### ğŸ’¾ DÃ¼zenli Ã‡Ä±ktÄ±
- **Data KlasÃ¶rÃ¼**: TÃ¼m sonuÃ§lar `data/` klasÃ¶rÃ¼ne kaydedilir
- **JSON Format**: YapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã§Ä±ktÄ± formatÄ±
- **Zaman DamgasÄ±**: Otomatik dosya adlandÄ±rma

## Kurulum

```bash
# Virtual environment oluÅŸtur
python3 -m venv venv
source venv/bin/activate

# KÃ¼tÃ¼phaneleri yÃ¼kle
pip install -r requirements.txt
```

## KullanÄ±m

### ğŸ•¸ï¸ Spider Crawl ModlarÄ±

#### 1. Normal Spider Crawl (Sadece Domain Zincirleme)
```bash
# Basit spider crawl - sadece domain seviyesinde zincirleme
python main.py https://pasha.org.tr --spider

# Derin spider crawl
python main.py https://pasha.org.tr --spider --spider-depth 5
```

#### 2. Enhanced Spider Crawl (Sayfa + Domain Zincirleme)
```bash
# GeliÅŸmiÅŸ spider crawl - hem sayfa hem domain seviyesinde zincirleme
python main.py https://pasha.org.tr --enhanced-spider

# Derin ve geniÅŸ enhanced spider crawl
python main.py https://pasha.org.tr --enhanced-spider \
    --spider-depth 4 \
    --spider-domains-per-level 15 \
    --spider-max-domains 200 \
    --spider-max-pages-per-domain 30
```

#### Spider Crawl KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Ã–zellik | Normal Spider | Enhanced Spider |
|---------|---------------|-----------------|
| Domain Zincirleme | âœ… | âœ… |
| Sayfa Zincirleme | âŒ | âœ… |
| Bulunan Domain SayÄ±sÄ± | Orta | Ã‡ok YÃ¼ksek |
| Bulunan Sayfa SayÄ±sÄ± | Az | Ã‡ok YÃ¼ksek |
| Ã‡alÄ±ÅŸma SÃ¼resi | HÄ±zlÄ± | Daha Uzun |
| Kaynak KullanÄ±mÄ± | Az | Orta |

### ğŸ­ Random User Agent

```bash
# Her istekte farklÄ± user agent kullan
python main.py https://pasha.org.tr --random-user-agent

# Oturum baÅŸÄ±na bir random user agent kullan
python main.py https://pasha.org.tr --random-user-agent-per-session

# Spider crawl ile random user agent
python main.py https://pasha.org.tr --spider --random-user-agent

# Enhanced spider crawl ile random user agent
python main.py https://pasha.org.tr --enhanced-spider --random-user-agent
```

### ğŸŒ Proxy DesteÄŸi

```bash
# Tek proxy kullanÄ±mÄ±
python main.py https://pasha.org.tr --proxy http://proxy.pasha.org.tr:8080

# Birden fazla proxy kullanÄ±mÄ±
python main.py https://pasha.org.tr --proxy http://proxy1:8080 socks5://proxy2:1080

# Proxy dosyasÄ±ndan yÃ¼kleme
python main.py https://pasha.org.tr --proxy-file proxies.txt

# Proxy rotasyonunu kapatma
python main.py https://pasha.org.tr --proxy http://proxy:8080 --no-proxy-rotation

# Spider crawl ile proxy
python main.py https://pasha.org.tr --spider --proxy http://proxy:8080

# Enhanced spider crawl ile proxy ve random user agent
python main.py https://pasha.org.tr --enhanced-spider --proxy-file proxies.txt --random-user-agent
```

### ğŸš« Domain Engelleme

```bash
# Belirli domain'leri engelle
python main.py https://pasha.org.tr --block-domains google.com facebook.com

# Domain engelleme dosyasÄ±ndan yÃ¼kle
python main.py https://pasha.org.tr --block-domains-file blocked_domains.txt

# VarsayÄ±lan engellenen domain'leri kullan
python main.py https://pasha.org.tr --use-default-blocked-domains

# Kombinasyon: Dosya + komut satÄ±rÄ± + varsayÄ±lan
python main.py https://pasha.org.tr --block-domains-file blocked_domains.txt --block-domains twitter.com --use-default-blocked-domains

# Spider crawl ile domain engelleme
python main.py https://pasha.org.tr --spider --block-domains google.com youtube.com

# Enhanced spider crawl ile domain engelleme
python main.py https://pasha.org.tr --enhanced-spider --block-domains-file blocked_domains.txt
```

### ğŸ“„ Dosya UzantÄ±sÄ± Filtreleme

```bash
# VarsayÄ±lan filtreleme (74 uzantÄ± hariÃ§ tutuluyor)
python main.py https://pasha.org.tr

# Ã–zel uzantÄ±larÄ± hariÃ§ tut
python main.py https://pasha.org.tr --exclude-extensions .pdf .jpg .png

# TÃ¼m dosya tÃ¼rlerini dahil et
python main.py https://pasha.org.tr --include-all-extensions

# Spider crawl ile Ã¶zel filtreleme
python main.py https://pasha.org.tr --spider --exclude-extensions .css .js
```

### ğŸ” Normal Domain Crawl

```bash
# Basit crawl
python main.py https://pasha.org.tr

# DetaylÄ± crawl
python main.py https://pasha.org.tr \
    --crawler-depth 3 \
    --crawler-max-urls 100 \
    --detector-timeout 15
```

### ğŸ Python Kodunda

```python
from domain_crawler import spider_crawl_domains

# Spider crawl baÅŸlat
results = spider_crawl_domains(
    initial_urls=['https://pasha.org.tr'],
    max_depth=3,
    max_domains_per_level=20,
    max_total_domains=100,
    excluded_extensions=['.pdf', '.jpg', '.png']  # Ã–zel filtreleme
)

print(f"Bulunan domain sayÄ±sÄ±: {results['total_domains_found']}")
for domain in results['valid_domains'][:10]:
    print(f"- {domain}")
```

## Spider Crawl NasÄ±l Ã‡alÄ±ÅŸÄ±r?

### Normal Spider Crawl (Sadece Domain)
```
Seviye 1: https://pasha.org.tr
    â†“ (crawl eder, .css/.js dosyalarÄ±nÄ± atlar)
    â†’ github.com, cdn.pasha.org.tr, api.pasha.org.tr

Seviye 2: https://github.com, https://cdn.pasha.org.tr, https://api.pasha.org.tr
    â†“ (her birini crawl eder, filtreleme uygular)
    â†’ microsoft.com, jquery.com, cloudflare.com, ...

Seviye 3: https://microsoft.com, https://jquery.com, ...
    â†“ (devam eder)
    â†’ Daha fazla domain...
```

### Enhanced Spider Crawl (Sayfa + Domain)
```
Seviye 1: https://pasha.org.tr
    â†“ (derin sayfa crawl + domain tespit)
    â†’ Domains: github.com, cdn.pasha.org.tr, api.pasha.org.tr
    â†’ Pages: /about, /contact, /blog, /products, ...

Seviye 2: Domain'ler + Sayfalar
    â†“ (hem yeni domain'leri hem yeni sayfalarÄ± crawl eder)
    â†’ https://github.com + https://pasha.org.tr/about + ...
    â†’ Ã‡ok daha fazla domain ve sayfa...

Seviye 3: Exponential Growth!
    â†“ (her seviyede katlanarak bÃ¼yÃ¼r)
    â†’ YÃ¼zlerce domain ve binlerce sayfa...
```

## Parametreler

### Ana Parametreler

- `urls`: Crawl edilecek baÅŸlangÄ±Ã§ URL'leri (zorunlu)
- `--spider`: Spider crawl modunu etkinleÅŸtir
- `--output, -o`: Ã‡Ä±ktÄ± dosyasÄ± adÄ± (data/ klasÃ¶rÃ¼ne kaydedilir)

### Spider Crawl Parametreleri

- `--spider`: Normal spider crawl modu (sadece domain zincirleme)
- `--enhanced-spider`: GeliÅŸmiÅŸ spider crawl modu (sayfa + domain zincirleme)
- `--spider-depth`: Spider crawl derinliÄŸi (varsayÄ±lan: 3)
- `--spider-domains-per-level`: Seviye baÅŸÄ±na max domain (varsayÄ±lan: 20)
- `--spider-max-domains`: Toplam max domain (varsayÄ±lan: 100)
- `--spider-max-pages-per-domain`: Domain baÅŸÄ±na max sayfa (enhanced spider iÃ§in, varsayÄ±lan: 50)

### Dosya Filtreleme Parametreleri

- `--exclude-extensions`: HariÃ§ tutulacak uzantÄ±lar (Ã¶rn: --exclude-extensions .pdf .jpg)
- `--include-all-extensions`: TÃ¼m dosya uzantÄ±larÄ±nÄ± dahil et

### Random User Agent Parametreleri

- `--random-user-agent`: Her istekte farklÄ± random user agent kullan
- `--random-user-agent-per-session`: Oturum baÅŸÄ±na bir random user agent kullan

### Proxy Parametreleri

- `--proxy`: KullanÄ±lacak proxy listesi (Ã¶rn: --proxy http://proxy1:8080 socks5://proxy2:1080)
- `--proxy-file`: Proxy listesini iÃ§eren dosya yolu
- `--no-proxy-rotation`: Proxy rotasyonunu devre dÄ±ÅŸÄ± bÄ±rak

### Domain Engelleme Parametreleri

- `--block-domains`: Engellenecek domain listesi (Ã¶rn: --block-domains google.com facebook.com)
- `--block-domains-file`: Engellenecek domain listesini iÃ§eren dosya yolu
- `--use-default-blocked-domains`: VarsayÄ±lan engellenen domain listesini kullan

### Crawler Parametreleri

- `--crawler-delay`: Crawler gecikme sÃ¼resi (varsayÄ±lan: 1.0)
- `--crawler-depth`: Crawler derinliÄŸi (varsayÄ±lan: 2)
- `--crawler-max-urls`: Crawler max URL (varsayÄ±lan: 50)

### Detector Parametreleri

- `--detector-delay`: Detector gecikme sÃ¼resi (varsayÄ±lan: 0.5)
- `--detector-timeout`: Detector timeout (varsayÄ±lan: 10)
- `--no-validation`: Domain doÄŸrulamasÄ±nÄ± devre dÄ±ÅŸÄ± bÄ±rak

## Ã‡Ä±ktÄ± FormatÄ±

**SonuÃ§lar `data/` klasÃ¶rÃ¼ne JSON formatÄ±nda kaydedilir:**

```json
{
  "total_domains_found": 87,
  "total_valid_domains": 85,
  "total_invalid_domains": 2,
  "valid_domains": [
    "github.com",
    "microsoft.com",
    "jquery.com",
    "cloudflare.com"
  ],
  "excluded_extensions": [".css", ".js", ".pdf", ".jpg"],
  "spider_depth": 3,
  "execution_time_seconds": 45.67,
  "timestamp": "2025-08-15T18:05:21.272Z"
}
```

## HÄ±zlÄ± Test

```bash
# Demo Ã§alÄ±ÅŸtÄ±r (varsayÄ±lan filtreleme ile)
python main.py https://pasha.org.tr

# Spider crawl test
python main.py https://pasha.org.tr --spider --spider-depth 2

# Ã–zel filtreleme test
python main.py https://pasha.org.tr --exclude-extensions .css .js --crawler-max-urls 20
```

## VarsayÄ±lan HariÃ§ Tutulan Dosya UzantÄ±larÄ±

Sistem varsayÄ±lan olarak 74 farklÄ± dosya uzantÄ±sÄ±nÄ± hariÃ§ tutar:

- **Resim**: .jpg, .jpeg, .png, .gif, .svg, .webp, .ico, .bmp, .tiff
- **Video**: .mp4, .avi, .mov, .wmv, .flv, .webm, .mkv
- **Ses**: .mp3, .wav, .flac, .aac, .ogg, .wma
- **DokÃ¼man**: .pdf, .doc, .docx, .xls, .xlsx, .ppt, .pptx
- **ArÅŸiv**: .zip, .rar, .tar, .gz, .7z, .bz2
- **Web**: .css, .js, .json, .xml, .scss, .sass
- **Font**: .ttf, .otf, .woff, .woff2, .eot
- **YazÄ±lÄ±m**: .exe, .msi, .dmg, .pkg, .deb, .rpm

## Spider Crawl AvantajlarÄ±

### Normal Spider Crawl
1. **GeniÅŸ Domain Kapsama**: Tek URL'den yÃ¼zlerce domain bulabilir
2. **AkÄ±llÄ± Filtreleme**: Gereksiz dosyalar crawl edilmez
3. **KontrollÃ¼**: Derinlik ve domain sayÄ±sÄ± limitleri
4. **HÄ±zlÄ±**: Dosya uzantÄ±sÄ± filtreleme ile performans artÄ±ÅŸÄ±
5. **GÃ¼venilir**: DNS doÄŸrulama ile sadece gerÃ§ek domain'ler

### Enhanced Spider Crawl
1. **Ã‡ift Zincirleme**: Hem domain hem sayfa seviyesinde zincirleme
2. **Exponential Growth**: Her seviyede katlanarak bÃ¼yÃ¼yen keÅŸif
3. **Derin Sayfa Analizi**: Her domain'den maksimum sayfa Ã§Ä±karÄ±mÄ±
4. **Hibrit Strateji**: Domain'ler + sayfalar birlikte iÅŸlenir
5. **Maksimum Kapsama**: Tek URL'den binlerce sayfa ve yÃ¼zlerce domain
6. **AkÄ±llÄ± Kaynak YÃ¶netimi**: Sayfa baÅŸÄ±na limit ile kontrollÃ¼ bÃ¼yÃ¼me

### GeliÅŸtirme OrtamÄ±

```bash
# Projeyi klonla
git clone https://github.com/Pashaorgtr/Spider.git
cd spider-domain-crawler

# Virtual environment oluÅŸtur
python3 -m venv venv
source venv/bin/activate

# Development dependencies yÃ¼kle
pip install -r requirements.txt

# Testleri Ã§alÄ±ÅŸtÄ±r
python main.py https://pasha.org.tr --crawler-max-urls 5
```

## ğŸ“ Changelog

### v1.0.0 (2025-08-17)
- âœ¨ Spider crawl Ã¶zelliÄŸi
- âœ¨ Enhanced spider crawl (sayfa + domain zincirleme)
- âœ¨ Random user agent desteÄŸi
- âœ¨ Proxy desteÄŸi (HTTP/HTTPS/SOCKS)
- âœ¨ Domain engelleme sistemi
- âœ¨ 74 dosya uzantÄ±sÄ± filtreleme
- âœ¨ JSON Ã§Ä±ktÄ± formatÄ±
- âœ¨ DNS domain doÄŸrulama

## ğŸ› Bilinen Sorunlar

- BazÄ± JavaScript-heavy siteler tam olarak crawl edilemeyebilir
- Ã‡ok bÃ¼yÃ¼k siteler iÃ§in memory kullanÄ±mÄ± artabilir
- Rate limiting nedeniyle bazÄ± siteler eriÅŸimi kÄ±sÄ±tlayabilir

## ğŸ“ Destek

- ğŸ› **Bug Report**: [Issues](https://github.com/Pashaorgtr/Spider/issues) sayfasÄ±ndan bildirebilirsiniz
- ğŸ’¡ **Feature Request**: Yeni Ã¶zellik Ã¶nerileri iÃ§in Issues kullanÄ±n
- ğŸ“§ **Ä°letiÅŸim**: [yasin@pasha.org.tr](mailto:yasin@pasha.org.tr)

## â­ YÄ±ldÄ±z Verin

Bu proje iÅŸinize yaradÄ±ysa, lÃ¼tfen â­ vererek destekleyin!

## ğŸ“„ Lisans

Bu proje PSH 1.1 (Pasha Software License) lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r. 

**Ã–nemli:** Bu yazÄ±lÄ±m **ticari kullanÄ±m iÃ§in kesinlikle yasaktÄ±r**. Ticari kullanÄ±m iÃ§in hiÃ§bir koÅŸulda lisans verilmeyecektir.

- **Gayri-ticari kullanÄ±m:** Tamamen Ã¼cretsiz (kiÅŸisel, akademik, topluluk projeleri)
- **Ticari kullanÄ±m:** Kesinlikle yasak - Ä°hlal durumunda %20 tazminat + 100.000 TL minimum ceza
- **AmaÃ§:** Telif hakkÄ± korumasÄ± ve ticari sÃ¶mÃ¼rÃ¼nÃ¼n engellenmesi

Detaylar iÃ§in [LICENSE.md](LICENSE.md) dosyasÄ±na bakÄ±n.

---

**Spider Domain Crawler** - Web'den domain keÅŸfetmenin en akÄ±llÄ± yolu ğŸ•·ï¸
