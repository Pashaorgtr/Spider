"""
Domain Detector Module
Web sayfalarÄ±ndan domain'leri tespit etme ve doÄŸrulama
"""

"""
Spider Domain Crawler - Domain Detector ModÃ¼lÃ¼

Copyright (c) 2025 Hasan Yasin YaÅŸar
Licensed under PSH 1.1 (Pasha Software License)
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import re
import json
import socket
import random
from typing import Set, List, Dict, Optional
import logging
import time

class DomainDetector:
    """Domain tespit etme ve doÄŸrulama sÄ±nÄ±fÄ±"""
    
    def __init__(self, delay: float = 1.0, timeout: int = 10, validate_domains: bool = True, 
                 use_random_user_agent: bool = False, use_proxy: bool = False, proxy_list: List[str] = None,
                 blocked_domains: List[str] = None, use_domain_blocking: bool = False):
        """
        DomainDetector baÅŸlatÄ±cÄ±
        
        Args:
            delay: Ä°stekler arasÄ± bekleme sÃ¼resi
            timeout: HTTP request timeout
            validate_domains: Domain'lerin gerÃ§ek olup olmadÄ±ÄŸÄ±nÄ± kontrol et
            use_random_user_agent: Random user agent kullan
            use_proxy: Proxy kullan
            proxy_list: KullanÄ±lacak proxy listesi
            blocked_domains: Engellenen domain listesi
            use_domain_blocking: Domain engelleme kullan
        """
        self.delay = delay
        self.timeout = timeout
        self.validate_domains = validate_domains
        self.use_random_user_agent = use_random_user_agent
        self.use_proxy = use_proxy
        self.proxy_list = proxy_list or []
        self.current_proxy = None
        self.failed_proxies: Set[str] = set()
        self.use_domain_blocking = use_domain_blocking
        self.blocked_domains = set()
        self.blocked_urls_count = 0
        self.session = requests.Session()
        self.found_domains: Set[str] = set()
        self.valid_domains: Set[str] = set()
        self.invalid_domains: Set[str] = set()
        self.domain_urls: Dict[str, Set[str]] = {}
        
        # Logging ayarla
        self.logger = logging.getLogger(__name__)
        
        # User-Agent ayarla
        self._setup_user_agent()
        
        # Proxy ayarla
        self._setup_proxy()
        
        # Domain engelleme ayarla
        self._setup_domain_blocking(blocked_domains)
    
    def _setup_user_agent(self):
        """User agent ayarlarÄ±nÄ± yapar"""
        if self.use_random_user_agent:
            from .config import RANDOM_USER_AGENTS, RANDOM_USER_AGENT_PER_SESSION
            if RANDOM_USER_AGENT_PER_SESSION:
                # Oturum baÅŸÄ±na bir user agent seÃ§
                self.session_user_agent = random.choice(RANDOM_USER_AGENTS)
                self.session.headers.update({'User-Agent': self.session_user_agent})
                self.logger.info(f"Random user agent seÃ§ildi (oturum): {self.session_user_agent}")
            else:
                # Her istekte farklÄ± user agent kullanÄ±lacak
                self.logger.info("Random user agent modu: Her istekte farklÄ± user agent")
        else:
            from .config import TARASSUT_USER_AGENT
            self.session.headers.update({'User-Agent': TARASSUT_USER_AGENT})
            self.logger.info(f"Sabit user agent: {TARASSUT_USER_AGENT}")
    
    def _get_random_user_agent(self) -> str:
        """Random user agent dÃ¶ndÃ¼rÃ¼r"""
        from .config import RANDOM_USER_AGENTS
        return random.choice(RANDOM_USER_AGENTS)
    
    def _update_user_agent_if_needed(self):
        """Gerekirse user agent'Ä± gÃ¼nceller"""
        if self.use_random_user_agent:
            from .config import RANDOM_USER_AGENT_PER_REQUEST
            if RANDOM_USER_AGENT_PER_REQUEST:
                new_user_agent = self._get_random_user_agent()
                self.session.headers.update({'User-Agent': new_user_agent})
                self.logger.debug(f"User agent gÃ¼ncellendi: {new_user_agent}")
    
    def _setup_proxy(self):
        """Proxy ayarlarÄ±nÄ± yapar"""
        if self.use_proxy and self.proxy_list:
            self.logger.info(f"Proxy modu etkin: {len(self.proxy_list)} proxy mevcut")
            self._select_proxy()
        elif self.use_proxy and not self.proxy_list:
            self.logger.warning("Proxy modu etkin ama proxy listesi boÅŸ!")
            self.use_proxy = False
        else:
            self.logger.info("Proxy kullanÄ±lmÄ±yor")
    
    def _select_proxy(self) -> Optional[str]:
        """KullanÄ±labilir bir proxy seÃ§er"""
        if not self.proxy_list:
            return None
        
        # BaÅŸarÄ±sÄ±z proxy'leri hariÃ§ tut
        available_proxies = [p for p in self.proxy_list if p not in self.failed_proxies]
        
        if not available_proxies:
            self.logger.warning("TÃ¼m proxy'ler baÅŸarÄ±sÄ±z! Proxy'siz devam ediliyor.")
            self.use_proxy = False
            return None
        
        # Random proxy seÃ§
        proxy = random.choice(available_proxies)
        self.current_proxy = proxy
        
        # Session'a proxy ayarla
        proxy_dict = self._parse_proxy(proxy)
        if proxy_dict:
            self.session.proxies.update(proxy_dict)
            self.logger.info(f"Proxy seÃ§ildi: {proxy}")
            return proxy
        else:
            self.failed_proxies.add(proxy)
            return self._select_proxy()  # BaÅŸka proxy dene
    
    def _parse_proxy(self, proxy_url: str) -> Optional[Dict[str, str]]:
        """Proxy URL'ini parse eder"""
        try:
            if proxy_url.startswith('http://') or proxy_url.startswith('https://'):
                return {
                    'http': proxy_url,
                    'https': proxy_url
                }
            elif proxy_url.startswith('socks4://') or proxy_url.startswith('socks5://'):
                return {
                    'http': proxy_url,
                    'https': proxy_url
                }
            else:
                # Format belirtilmemiÅŸse HTTP olarak varsay
                formatted_proxy = f"http://{proxy_url}"
                return {
                    'http': formatted_proxy,
                    'https': formatted_proxy
                }
        except Exception as e:
            self.logger.error(f"Proxy parse hatasÄ± {proxy_url}: {e}")
            return None
    
    def _rotate_proxy(self):
        """Proxy rotasyonu yapar"""
        if self.use_proxy and self.proxy_list:
            from .config import PROXY_ROTATION
            if PROXY_ROTATION:
                old_proxy = self.current_proxy
                new_proxy = self._select_proxy()
                if new_proxy and new_proxy != old_proxy:
                    self.logger.debug(f"Proxy deÄŸiÅŸtirildi: {old_proxy} -> {new_proxy}")
    
    def _handle_proxy_failure(self):
        """Proxy baÅŸarÄ±sÄ±zlÄ±ÄŸÄ±nÄ± iÅŸler"""
        if self.current_proxy:
            self.failed_proxies.add(self.current_proxy)
            self.logger.warning(f"Proxy baÅŸarÄ±sÄ±z olarak iÅŸaretlendi: {self.current_proxy}")
            
            # Yeni proxy dene
            new_proxy = self._select_proxy()
            if not new_proxy:
                self.logger.warning("KullanÄ±labilir proxy kalmadÄ±, proxy'siz devam ediliyor")
                self.use_proxy = False
                self.session.proxies.clear()
    
    def _setup_domain_blocking(self, blocked_domains: List[str] = None):
        """Domain engelleme ayarlarÄ±nÄ± yapar"""
        if not self.use_domain_blocking:
            self.logger.info("Domain engelleme kullanÄ±lmÄ±yor")
            return
        
        # Engellenen domain'leri ayarla
        if blocked_domains:
            self.blocked_domains.update(domain.lower().strip() for domain in blocked_domains)
            self.logger.info(f"Domain engelleme: {len(blocked_domains)} domain komut satÄ±rÄ±ndan eklendi")
        else:
            from .config import DEFAULT_BLOCKED_DOMAINS
            self.blocked_domains.update(domain.lower().strip() for domain in DEFAULT_BLOCKED_DOMAINS)
            self.logger.info(f"Domain engelleme: {len(DEFAULT_BLOCKED_DOMAINS)} varsayÄ±lan domain eklendi")
        
        self.logger.info(f"Toplam engellenen domain sayÄ±sÄ±: {len(self.blocked_domains)}")
    
    def _is_domain_blocked(self, url: str) -> bool:
        """URL'nin domain'inin engellenip engellenmediÄŸini kontrol eder"""
        if not self.use_domain_blocking or not self.blocked_domains:
            return False
        
        try:
            from .config import DOMAIN_BLOCKING_MODE
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # Port numarasÄ±nÄ± kaldÄ±r
            if ':' in domain:
                domain = domain.split(':')[0]
            
            # www. prefix'ini kaldÄ±r
            if domain.startswith('www.'):
                domain = domain[4:]
            
            for blocked_domain in self.blocked_domains:
                if DOMAIN_BLOCKING_MODE == 'exact':
                    # Tam eÅŸleÅŸme
                    if domain == blocked_domain:
                        return True
                elif DOMAIN_BLOCKING_MODE == 'subdomain':
                    # Alt domain dahil
                    if domain == blocked_domain or domain.endswith('.' + blocked_domain):
                        return True
                elif DOMAIN_BLOCKING_MODE == 'contains':
                    # Ä°Ã§eren
                    if blocked_domain in domain:
                        return True
            
            return False
            
        except Exception as e:
            self.logger.debug(f"Domain engelleme kontrolÃ¼ hatasÄ± {url}: {e}")
            return False
    
    def _log_blocked_domain(self, url: str):
        """Engellenen domain'i loglar"""
        self.blocked_urls_count += 1
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        self.logger.info(f"ğŸš« Domain engellendi: {domain} (URL: {url})")
        self.logger.debug(f"Toplam engellenen URL sayÄ±sÄ±: {self.blocked_urls_count}")
    
    def extract_domain(self, url: str) -> str:
        """URL'den domain Ã§Ä±karÄ±r"""
        try:
            parsed = urlparse(url)
            return parsed.netloc.lower()
        except:
            return ""
    
    def is_valid_domain_format(self, domain: str) -> bool:
        """Domain formatÄ±nÄ±n geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol eder"""
        if not domain:
            return False
        
        # Basit domain regex kontrolÃ¼
        domain_pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*$'
        return bool(re.match(domain_pattern, domain))
    
    def validate_domain_exists(self, domain: str) -> bool:
        """Domain'in gerÃ§ekten var olup olmadÄ±ÄŸÄ±nÄ± kontrol eder"""
        if not self.validate_domains:
            return True
            
        try:
            # DNS lookup ile domain'in var olup olmadÄ±ÄŸÄ±nÄ± kontrol et
            socket.gethostbyname(domain)
            return True
        except socket.gaierror:
            # DNS Ã§Ã¶zÃ¼mlenemedi, domain geÃ§ersiz
            return False
        except Exception as e:
            self.logger.warning(f"Domain doÄŸrulama hatasÄ± {domain}: {e}")
            return False
    
    def get_page_content(self, url: str) -> BeautifulSoup:
        """Sayfa iÃ§eriÄŸini alÄ±r"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # Random user agent kullanÄ±lÄ±yorsa gÃ¼ncelle
                self._update_user_agent_if_needed()
                
                # Proxy rotasyonu
                if attempt > 0:  # Ä°lk denemede rotasyon yapma
                    self._rotate_proxy()
                
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                
                content_type = response.headers.get('content-type', '').lower()
                if 'text/html' not in content_type:
                    return None
                
                return BeautifulSoup(response.content, 'html.parser')
                
            except requests.exceptions.ProxyError as e:
                self.logger.warning(f"Proxy hatasÄ± {url} (Deneme {attempt + 1}/{max_retries}): {e}")
                self._handle_proxy_failure()
                if attempt == max_retries - 1:
                    self.logger.error(f"TÃ¼m proxy denemeleri baÅŸarÄ±sÄ±z: {url}")
                    return None
                continue
                
            except requests.exceptions.Timeout as e:
                self.logger.warning(f"Timeout {url} (Deneme {attempt + 1}/{max_retries}): {e}")
                if self.use_proxy:
                    self._rotate_proxy()
                if attempt == max_retries - 1:
                    return None
                continue
                
            except requests.RequestException as e:
                self.logger.error(f"Sayfa alÄ±namadÄ± {url} (Deneme {attempt + 1}/{max_retries}): {e}")
                if self.use_proxy and "proxy" in str(e).lower():
                    self._handle_proxy_failure()
                if attempt == max_retries - 1:
                    return None
                continue
        
        return None
    
    def extract_domains_from_page(self, url: str, soup: BeautifulSoup) -> Set[str]:
        """Sayfa iÃ§eriÄŸinden domain'leri Ã§Ä±karÄ±r"""
        domains = set()
        
        # <a> etiketlerinden href'leri al
        for link in soup.find_all('a', href=True):
            href = link['href'].strip()
            if href:
                absolute_url = urljoin(url, href)
                domain = self.extract_domain(absolute_url)
                
                if self.is_valid_domain_format(domain):
                    domains.add(domain)
                    
                    if domain not in self.domain_urls:
                        self.domain_urls[domain] = set()
                    self.domain_urls[domain].add(absolute_url)
        
        # <link> etiketlerinden href'leri al
        for link in soup.find_all('link', href=True):
            href = link['href'].strip()
            if href:
                absolute_url = urljoin(url, href)
                domain = self.extract_domain(absolute_url)
                
                if self.is_valid_domain_format(domain):
                    domains.add(domain)
                    
                    if domain not in self.domain_urls:
                        self.domain_urls[domain] = set()
                    self.domain_urls[domain].add(absolute_url)
        
        # <img> etiketlerinden src'leri al
        for img in soup.find_all('img', src=True):
            src = img['src'].strip()
            if src:
                absolute_url = urljoin(url, src)
                domain = self.extract_domain(absolute_url)
                
                if self.is_valid_domain_format(domain):
                    domains.add(domain)
                    
                    if domain not in self.domain_urls:
                        self.domain_urls[domain] = set()
                    self.domain_urls[domain].add(absolute_url)
        
        # <script> etiketlerinden src'leri al
        for script in soup.find_all('script', src=True):
            src = script['src'].strip()
            if src:
                absolute_url = urljoin(url, src)
                domain = self.extract_domain(absolute_url)
                
                if self.is_valid_domain_format(domain):
                    domains.add(domain)
                    
                    if domain not in self.domain_urls:
                        self.domain_urls[domain] = set()
                    self.domain_urls[domain].add(absolute_url)
        
        return domains
    
    def detect_domains_from_url(self, url: str) -> Set[str]:
        """Tek bir URL'den domain'leri tespit eder"""
        # Domain engelleme kontrolÃ¼
        if self._is_domain_blocked(url):
            self._log_blocked_domain(url)
            return set()
        
        self.logger.info(f"Domain tespiti yapÄ±lÄ±yor: {url}")
        
        # Sayfa iÃ§eriÄŸini al
        soup = self.get_page_content(url)
        if not soup:
            return set()
        
        # Domain'leri Ã§Ä±kar
        domains = self.extract_domains_from_page(url, soup)
        
        # Bulunan domain'leri kaydet (engelleme kontrolÃ¼ ile)
        for domain in domains:
            # Domain'in kendisini kontrol et (URL formatÄ±nda)
            test_url = f"https://{domain}"
            if not self._is_domain_blocked(test_url):
                if domain not in self.found_domains:
                    self.found_domains.add(domain)
                    self.logger.info(f"Yeni domain bulundu: {domain}")
            else:
                self.logger.info(f"ğŸš« Domain engellendi: {domain}")
        
        return domains
    
    def validate_all_domains(self) -> None:
        """Bulunan tÃ¼m domain'leri doÄŸrular"""
        if not self.validate_domains:
            self.valid_domains = self.found_domains.copy()
            return
            
        self.logger.info(f"Domain doÄŸrulama baÅŸlatÄ±lÄ±yor ({len(self.found_domains)} domain)...")
        
        for i, domain in enumerate(self.found_domains, 1):
            self.logger.info(f"DoÄŸrulanÄ±yor ({i}/{len(self.found_domains)}): {domain}")
            
            if self.validate_domain_exists(domain):
                self.valid_domains.add(domain)
                self.logger.info(f"âœ… GeÃ§erli domain: {domain}")
            else:
                self.invalid_domains.add(domain)
                self.logger.warning(f"âŒ GeÃ§ersiz domain: {domain}")
            
            # Rate limiting
            if i < len(self.found_domains):
                time.sleep(0.1)  # DNS sorgularÄ± iÃ§in kÄ±sa bekleme
        
        self.logger.info(f"Domain doÄŸrulama tamamlandÄ±. GeÃ§erli: {len(self.valid_domains)}, GeÃ§ersiz: {len(self.invalid_domains)}")
    
    def detect_domains_from_urls(self, urls: List[str]) -> Dict:
        """Birden fazla URL'den domain'leri tespit eder"""
        self.logger.info(f"Toplam {len(urls)} URL'den domain tespiti baÅŸlatÄ±lÄ±yor")
        
        for i, url in enumerate(urls, 1):
            self.logger.info(f"Ä°ÅŸleniyor ({i}/{len(urls)}): {url}")
            
            try:
                self.detect_domains_from_url(url)
                
                # Rate limiting
                if i < len(urls):
                    time.sleep(self.delay)
                    
            except Exception as e:
                self.logger.error(f"URL iÅŸlenirken hata {url}: {e}")
                continue
        
        # Domain doÄŸrulama
        self.validate_all_domains()
        
        # SonuÃ§larÄ± hazÄ±rla
        results = {
            'total_domains_found': len(self.found_domains),
            'total_valid_domains': len(self.valid_domains),
            'total_invalid_domains': len(self.invalid_domains),
            'total_urls_processed': len(urls),
            'valid_domains': sorted(list(self.valid_domains)),
            'invalid_domains': sorted(list(self.invalid_domains)) if self.validate_domains else [],
            'validation_enabled': self.validate_domains,
            'analysis_time': time.time()
        }
        
        self.logger.info(f"Domain tespiti tamamlandÄ±. GeÃ§erli: {len(self.valid_domains)} domain bulundu.")
        return results
    
    def filter_domains(self, 
                      domains: List[str] = None,
                      exclude_domains: List[str] = None,
                      tld_filter: List[str] = None) -> List[str]:
        """Domain'leri filtreler"""
        
        if domains is None:
            domains = list(self.valid_domains)
        
        exclude_domains = exclude_domains or []
        tld_filter = tld_filter or []
        
        filtered_domains = []
        
        for domain in domains:
            # HariÃ§ tutulacak domain'ler
            if any(excluded in domain for excluded in exclude_domains):
                continue
            
            # TLD filtresi
            if tld_filter:
                domain_tld = domain.split('.')[-1] if '.' in domain else ''
                if domain_tld not in tld_filter:
                    continue
            
            filtered_domains.append(domain)
        
        return filtered_domains
    
    def save_results(self, results: Dict, filename: str = 'domains.json') -> None:
        """SonuÃ§larÄ± JSON dosyasÄ±na kaydet - sadece domain'ler"""
        
        # Sadece geÃ§erli domain'leri kaydet
        simple_results = {
            'domains': results['valid_domains'],
            'total_count': results['total_valid_domains'],
            'validation_enabled': results['validation_enabled'],
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # EÄŸer doÄŸrulama kapalÄ±ysa, geÃ§ersiz domain bilgisini de ekle
        if not results['validation_enabled']:
            simple_results['note'] = 'Domain validation disabled - all found domains included'
        else:
            simple_results['invalid_domains_count'] = results['total_invalid_domains']
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(simple_results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"Domain'ler {filename} dosyasÄ±na kaydedildi.")
    
    def print_summary(self, results: Dict) -> None:
        """SonuÃ§larÄ±n Ã¶zetini yazdÄ±rÄ±r"""
        print(f"\n=== DOMAIN TESPÄ°T SONUÃ‡LARI ===")
        print(f"Bulunan toplam domain: {results['total_domains_found']}")
        print(f"GeÃ§erli domain'ler: {results['total_valid_domains']}")
        
        if results['validation_enabled']:
            print(f"GeÃ§ersiz domain'ler: {results['total_invalid_domains']}")
        else:
            print("Domain doÄŸrulama: KapalÄ±")
        
        print(f"Ä°ÅŸlenen URL sayÄ±sÄ±: {results['total_urls_processed']}")
        
        print(f"\n=== GEÃ‡ERLÄ° DOMAIN'LER ===")
        for i, domain in enumerate(results['valid_domains'][:20], 1):
            print(f"{i}. {domain}")
        
        if len(results['valid_domains']) > 20:
            print(f"... ve {len(results['valid_domains']) - 20} domain daha")
        
        if results['validation_enabled'] and results['invalid_domains']:
            print(f"\n=== GEÃ‡ERSÄ°Z DOMAIN'LER (Ä°lk 5) ===")
            for i, domain in enumerate(results['invalid_domains'][:5], 1):
                print(f"{i}. {domain}")
            
            if len(results['invalid_domains']) > 5:
                print(f"... ve {len(results['invalid_domains']) - 5} geÃ§ersiz domain daha")
